<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Pushing the Limits of 3D Shape Generation at Scale</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://naiq.github.io">Xuelin Qian</a><sup></sup>,</span>
            <span class="author-block">
              <a href="">Yu Wang</a><sup></sup>,</span>
            <span class="author-block">
              <a href="https://luosiallen.github.io">Simian Luo</a><sup></sup>,</span>
            <span class="author-block">
              <a href="https://www.zhangyinda.com">Yinda Zhang</a><sup></sup>,</span>
            <span class="author-block">
              <a href="https://tyshiwo.github.io">Ying Tai</a><sup></sup>,</span>
            <span class="author-block">
              <a href="https://jessezhang92.github.io">Zhenyu Zhang</a><sup></sup>,</span>
            <span class="author-block">
              <a href="">Chengjie Wang</a><sup></sup>,</span>
            <span class="author-block">
              <a href="">Xiangyang Xue</a><sup></sup>,</span>
            <span class="author-block">
              <a href="https://www.bozhao.me">Bo Zhao</a><sup></sup>,</span>
            <span class="author-block">
              <a href="">Tiejun Huang</a><sup></sup>,</span>
            <span class="author-block">
              <a href="">Yunsheng Wu</a><sup></sup>,</span>
            <span class="author-block">
              <a href="https://yanweifu.github.io">Yanwei Fu</a><sup></sup></span>
<!--            <span class="author-block">-->
<!--              <a href="https://yanweifu.github.io">Ricardo Martin-Brualla</a><sup>2</sup>-->
<!--            </span>-->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup></sup>Fudan University</span>
            <span class="author-block"><sup></sup>&nbsp&nbsp&nbsp Tsinghua University</span>
            <span class="author-block"><sup></sup>&nbsp&nbsp&nbsp Tencent Youtu lab</span>
            <span class="author-block"><sup></sup>&nbsp&nbsp&nbsp Beijing Academy of Artificial Intelligence</span>
            <span class="author-block"><sup></sup>&nbsp&nbsp&nbsp Google</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.12225"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/FVPLab/Argus-3D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/BAAI/Objaverse-MIX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>ðŸ¤— Dataset</span>
                  </a>
              </span>
              <!-- Dataset Link. -->

          </div>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="flex-row">
      <figure style="width: 100%;">
        <a>
          <img width="100%" src="static/images/primary.png">
        </a>
      </figure>
      <p style="text-align:justify">
          Auto-regressive models have achieved impressive results in 2D image generation by modeling joint distributions in grid space.
          In this paper, we extend auto-regressive models to 3D domains, and seek a stronger ability of 3D shape generation by improving
          auto-regressive models at capacity and scalability simultaneously. Firstly, we leverage an ensemble of publicly available 3D datasets
          to facilitate the training of large-scale models. It consists of a comprehensive collection of approximately 900,000 objects, with
          multiple properties of meshes, points, voxels, rendered images, and text captions.
          This diverse <i>labeled</i> dataset, termed <b><i>Objaverse-Mix</i></b>, empowers our model to learn from a wide range of object
          variations. For data processing, we employ four machines with 64-core CPUs and 8 A100 GPUs each over four weeks, utilizing nearly 100TB
          of storage due to process complexity.
      </p>
      <p style="text-align:justify">
          However, directly applying 3D auto-regression encounters critical challenges of high computational demands on volumetric grids and
          ambiguous auto-regressive order along grid dimensions, resulting in inferior quality of 3D shapes. To this end, we then present a
          novel framework <b><i>Argus3D</i></b> in terms of capacity.
          Concretely, our approach introduces discrete representation learning based on a latent vector instead of volumetric grids, which
          not only reduces computational costs but also preserves essential geometric details by learning the joint distributions in a more
          tractable order. The capacity of conditional generation can thus be realized by simply concatenating various conditioning inputs
          to the latent vector, such as point clouds, categories, images, and texts. In addition, thanks to the simplicity of our model
          architecture, we naturally scale up our approach to a larger model with an impressive 3.6 billion parameters, further enhancing
          the quality of versatile 3D generation.
      </p>
      <p style="text-align:justify">
          Extensive experiments on four generation tasks demonstrate that Argus3D can synthesize diverse and faithful shapes across multiple
          categories, achieving remarkable performance.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
<!--        <h2 class="title is-3">Generation</h2>-->

        <!-- Interpolating. -->
        <h3 class="title is-3">Class-guide generation</h3>
        <div class="content has-text-justified">
          <p style="text-align:center">
<!--            Our huge model can generate high quality and rich diversity meshes.-->
          </p>
        </div>
        <figure>
          <video class="centered" width="100%" autoplay="" loop="" muted="" playsinline="" >
            <source src="static/videos/plane.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="caption" style="text-align:center">
            Generated meshes of planes
          </p>
        </figure>
        <br/>
          <figure>
              <video class="centered" width="100%" autoplay="" loop="" muted="" playsinline="" >
                  <source src="static/videos/table.mp4" type="video/mp4">
                  Your browser does not support the video tag.
              </video>
              <p class="caption" style="text-align:center">
                  Generated meshes of tables
              </p>
          </figure>
          <br/>
        <figure>
          <video class="centered" width="100%" autoplay="" loop="" muted="" playsinline="" >
            <source src="static/videos/car.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="caption" style="text-align:center">
            Generated meshes of cars
          </p>
        </figure>
        <br/>
        <figure>
          <video class="centered" width="100%" autoplay="" loop="" muted="" playsinline="" >
            <source src="static/videos/chair.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="caption" style="text-align:center">
            Generated meshes of chairs
          </p>
        </figure>
        <br/>
        <br/>
        <div class="g-scroll">
          <div class="g-pesudo">
            <img height="400px" src="static/images/test.png" >
          </div>
        </div>
        <p class="caption" style="text-align:center">
          Scroll to see more generate sample
        </p>
        <br/>
        <br/>
        <!--/ Image-guide generation. -->
        <h3 class="title is-3">Image-guide generation</h3>
        <figure style="text-align:center">
          <a>
            <img width="70%"  src="static/images/img_guide.png">
          </a>
          <div class="content has-text-justified">
            <p class="caption" style="text-align:center">
              Generated meshes from input image
            </p>
          </div>

        </figure>
        <br/>

        <!--/ Text-guide generation. -->
        <h3 class="title is-3">Text-guide generation</h3>
        <figure style="text-align:center">
          <a>
            <img width="70%"  src="static/images/text_guide.png">
          </a>
          <div class="content has-text-justified">
            <p class="caption" style="text-align:center">
              Generated meshes from input text prompt
            </p>
          </div>
        </figure>
        <br/>

        <!-- Re-rendering. -->
        <h3 class="title is-3">Texture</h3>
        <div class="content has-text-justified">
          <p >
              Image-guide generation based on DALLÂ·EÂ·2 generated images. Argus3D is capable of generating a wide range of shapes from unseen images.
              These shapes can be further enhanced with textures created by a texture model, which utilizes text prompts from DALLÂ·EÂ·2.
              Additionally, the use of Generated various text image prompts enables the generation of new and unique textures.
<!--            The generated meshes by our model can be textured by using state-of-the-arts methods like <a href="https://github.com/TEXTurePaper/TEXTurePaper">TEXTure</a>.-->
          </p>
        </div>
        <figure style="width: 100%;">
          <a>
            <img width="100%" src="static/images/dalle_pami.jpg">
          </a>
        </figure>        <!--/ Re-rendering. -->

        <h3 class="title is-3">Retrieval</h3>
        <div class="content has-text-justified">
          <p >
              Novel shape generation vs nearest neighbor retrieval (Followed <a href="https://ziyaerkoc.com/hyperdiffusion/">HyperDiffusion</a>). For generated shapes (left) from our method, we look up the top-5 nearest neighbors (right) from the training set based on the Chamfer distance. As shown, our method does not simply memorize train samples and can generalize to novel shapes.
          </p>
        </div>
        <figure style="text-align:center">
          <a>
            <img width="80%" src="static/images/re.png">
          </a>
        </figure>

      </div>
    </div>


  </div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{inproceedings,
      author = {Luo, Simian and Qian, Xuelin and Fu, Yanwei and Zhang, Yinda and Tai, Ying and Zhang, Zhenyu and Wang, Chengjie and Xue, Xiangyang},
      year = {2023},
      month = {10},
      pages = {14093-14103},
      title = {Learning Versatile 3D Shape Generation with Improved Auto-regressive Models},
      doi = {10.1109/ICCV51070.2023.01300}
      }

@misc{qian2024pushing,
      title={Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability},
      author={Xuelin Qian and Yu Wang and Simian Luo and Yinda Zhang and Ying Tai and Zhenyu Zhang and Chengjie Wang and Xiangyang Xue and Bo Zhao and Tiejun Huang and Yunsheng Wu and Yanwei Fu},
      year={2024},
      eprint={2402.12225},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
      }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://arxiv.org/abs/2402.12225">
        <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg="">
          <path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path>
        </svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
      </a>
      <a class="icon-link" href="https://github.com/FVPLab/Argus-3D" disabled="">
        <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align:center">
            Source code mainly borrowed from <a href="https://keunhong.com/">Keunhong Park</a>'s
            <a href="https://nerfies.github.io/">Nerfies website</a> and
            <a href="https://meshdiffusion.github.io">	MeshDiffusion</a>. Thanks for their beautiful website!
          </p>
        </div>
      </div>
    </div>  </div>
</footer>

</body>
</html>
